{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 05:33:33 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n"
     ]
    }
   ],
   "source": [
    "#pip install pororo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pororo import Pororo\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Available tasks are ['mrc', 'rc', 'qa', 'question_answering', 'machine_reading_comprehension', 'reading_comprehension', 'sentiment', 'sentiment_analysis', 'nli', 'natural_language_inference', 'inference', 'fill', 'fill_in_blank', 'fib', 'para', 'pi', 'cse', 'contextual_subword_embedding', 'similarity', 'sts', 'semantic_textual_similarity', 'sentence_similarity', 'sentvec', 'sentence_embedding', 'sentence_vector', 'se', 'inflection', 'morphological_inflection', 'g2p', 'grapheme_to_phoneme', 'grapheme_to_phoneme_conversion', 'w2v', 'wordvec', 'word2vec', 'word_vector', 'word_embedding', 'tokenize', 'tokenise', 'tokenization', 'tokenisation', 'tok', 'segmentation', 'seg', 'mt', 'machine_translation', 'translation', 'pos', 'tag', 'pos_tagging', 'tagging', 'const', 'constituency', 'constituency_parsing', 'cp', 'pg', 'collocation', 'collocate', 'col', 'word_translation', 'wt', 'summarization', 'summarisation', 'text_summarization', 'text_summarisation', 'summary', 'gec', 'review', 'review_scoring', 'lemmatization', 'lemmatisation', 'lemma', 'ner', 'named_entity_recognition', 'entity_recognition', 'zero-topic', 'dp', 'dep_parse', 'caption', 'captioning', 'asr', 'speech_recognition', 'st', 'speech_translation', 'tts', 'text_to_speech', 'speech_synthesis', 'ocr', 'srl', 'semantic_role_labeling', 'p2g', 'aes', 'essay', 'qg', 'question_generation', 'age_suitability', 'wsd']\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pororo.available_tasks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-25 08:07:14 | INFO | pororo.models.brainbert.tasks.sequence_tagging | [input] dictionary: 4005 types\n",
      "2022-11-25 08:07:14 | INFO | pororo.models.brainbert.tasks.sequence_tagging | [label] dictionary: 41 types\n"
     ]
    }
   ],
   "source": [
    "ner = Pororo(task=\"ner\", lang=\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Something', 'ARTIFACT'), ('〉는', 'O'), (' ', 'O'), ('조지 해리슨', 'PERSON'), ('이', 'O'), (' ', 'O'), ('쓰고', 'O'), (' ', 'O'), ('비틀즈', 'PERSON'), ('가', 'O'), (' ', 'O'), ('1969년', 'DATE'), (' ', 'O'), ('앨범', 'O'), (' ', 'O'), ('《', 'O'), ('Abbey Road', 'ARTIFACT'), ('》에', 'O'), (' ', 'O'), ('담은', 'O'), (' ', 'O'), ('노래다.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from itertools import permutations\n",
    "temp_list =  ner('Something〉는 조지 해리슨이 쓰고 비틀즈가 1969년 앨범 《Abbey Road》에 담은 노래다.')\n",
    "print(temp_list)\n",
    "\n",
    "remove_temp_list = [(i[0],i[1])  for i in temp_list if i[1] != 'O' ]\n",
    "#print(remove_temp_list)\n",
    "\n",
    "temp = []\n",
    "for idx , item in enumerate(permutations(['word'], 2)) :\n",
    "    if item[0][1] == 'PERSON' :\n",
    "        temp.append(item)\n",
    "\n",
    "for i in temp :\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import pathlib\n",
    "import pickle\n",
    "from itertools import combinations , permutations\n",
    "\n",
    "def num_to_label(label: np.ndarray) -> list:\n",
    "    \"\"\"\n",
    "    숫자로 되어 있던 class를 원본 문자열 라벨로 변환 합니다.\n",
    "    \"\"\"\n",
    "    origin_label = []\n",
    "    with open(\"/opt/ml/code/dict_num_to_label.pkl\", \"rb\") as f:\n",
    "        dict_num_to_label = pickle.load(f)\n",
    "    for v in label:\n",
    "        origin_label.append(dict_num_to_label[v])\n",
    "\n",
    "    return origin_label\n",
    "class RE_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset 구성을 위한 class.\"\"\"\n",
    "\n",
    "    def __init__(self, pair_dataset: pd.DataFrame, labels: np.ndarray):\n",
    "        self.pair_dataset = pair_dataset\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
    "        item = {\n",
    "            key: val[idx].clone().detach() for key, val in self.pair_dataset.items()\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class BaseDataLoader:\n",
    "    \"\"\"BaseLine DataLoader 입니다.\n",
    "\n",
    "    Args:\n",
    "        load_data :  (str): 가져올 데이터의 주소입니다.\n",
    "        tokenizer (AutoTokenizer): 데이터를 토큰화할 토크나이저입니다.\n",
    "\n",
    "    func :\n",
    "        1)preprocessing_dataset(pd.DataFrame) : 데이터 전처리를 합니다 return pd.DataFrame\n",
    "        2)tokenized_dataset(pd.DataFrame, tokenizer: AutoTokenizer) : 전처리된 데이터셋을 tokenized_dataset으로 반환 합니다\n",
    "        3)get_dataset : train, valid 데이터셋을 토큰화 셋으로 변환한 뒤 RE_Dataset 형태로 반환 합니다\n",
    "        4)get_test_dataset :  test 데이터셋을 토큰화 셋으로 변환한 뒤 RE_Dataset 형태로 반환 합니다\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data_path: pathlib.Path, tokenizer: AutoTokenizer):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def preprocessing_dataset(self, dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"처음 불러온 csv 파일을 원하는 형태의 DataFrame으로 변경 시켜줍니다.\"\"\"\n",
    "        subject_entity = []\n",
    "        object_entity = []\n",
    "\n",
    "        re_idx = []\n",
    "        start = 0\n",
    "        sentence = []\n",
    "        label =[]\n",
    "        sub_list = ['PERSON' , 'ORGANIZATION']\n",
    "        obj_list = ['PERSON' , 'LOCATION', 'ORGANIZATION', 'ARTIFACT', 'DATE' , 'CIVILIZATION']\n",
    "\n",
    "        print(list( set(sub_list) | set(obj_list)))\n",
    "\n",
    "        for row in dataset[:]['sentence'] :\n",
    "            try :\n",
    "                pororo_to_sentence = list(set(ner(row)))\n",
    "            except :\n",
    "                continue\n",
    "            pororo_to_sentence = [(i[0],i[1])  for i in pororo_to_sentence if i[1] in list(set(sub_list) | set(obj_list))]\n",
    "            for idx , item in enumerate(combinations(pororo_to_sentence, 2)) :\n",
    "                if (item[0][1] in sub_list) and (item[1][1] in obj_list)  :\n",
    "                    re_idx.append(start)\n",
    "                    sentence.append(row)\n",
    "                    subject_entity.append(item[0][0])\n",
    "                    object_entity.append(item[1][0])\n",
    "                    label.append(100)\n",
    "                    start = start +1\n",
    "\n",
    "        out_dataset = pd.DataFrame(\n",
    "            {\n",
    "                \"id\": re_idx,\n",
    "                \"sentence\": sentence,\n",
    "                \"subject_entity\": subject_entity,\n",
    "                \"object_entity\": object_entity,\n",
    "                \"label\": label,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return out_dataset\n",
    "\n",
    "    def tokenized_dataset(\n",
    "        self, dataset: pd.DataFrame, tokenizer: AutoTokenizer\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"tokenizer에 따라 sentence를 tokenizing 합니다.\"\"\"\n",
    "        concat_entity = []\n",
    "        for e01, e02 in zip(dataset[\"subject_entity\"], dataset[\"object_entity\"]):\n",
    "            temp = \"\"\n",
    "            temp = e01 + \"[SEP]\" + e02\n",
    "            concat_entity.append(temp)\n",
    "\n",
    "        tokenized_sentences = tokenizer(\n",
    "            concat_entity,\n",
    "            list(dataset[\"sentence\"]),\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "        return tokenized_sentences\n",
    "\n",
    "\n",
    "    def get_test_dataset(self) -> RE_Dataset:\n",
    "        \"\"\"데이터셋을 Trainer에 넣을 수 있도록 처리하여 리턴합니다.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): 가져올 데이터의 주소입니다.\n",
    "            tokenizer (AutoTokenizer): 데이터를 토큰화할 토크나이저입니다.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: _description_\n",
    "        \"\"\"\n",
    "        pd_dataset = pd.read_csv(self.data_path)\n",
    "        dataset = self.preprocessing_dataset(pd_dataset[:1000])\n",
    "        pre_sentence = dataset['sentence']\n",
    "        pre_sub_word = dataset['subject_entity']\n",
    "        pre_obj_word = dataset['object_entity']\n",
    "\n",
    "        dataset_label = list(map(int, dataset[\"label\"].values))\n",
    "        dataset_id = dataset[\"id\"]\n",
    "\n",
    "        # tokenizing dataset\n",
    "        dataset_tokens = self.tokenized_dataset(dataset, self.tokenizer)\n",
    "        # make dataset for pytorch.\n",
    "        dataset = RE_Dataset(dataset_tokens, dataset_label)\n",
    "        return dataset_id, dataset, dataset_label , pre_sentence , pre_sub_word , pre_obj_word\n",
    "\n",
    "def inference(model, tokenized_sent, batch_size, device):\n",
    "    \"\"\"\n",
    "    test dataset을 DataLoader로 만들어 준 후,\n",
    "    batch_size로 나눠 model이 예측 합니다.\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(tokenized_sent, batch_size, shuffle=False)\n",
    "    model.eval()\n",
    "\n",
    "    output_pred = []\n",
    "    output_prob = []\n",
    "    for i, data in enumerate(tqdm(dataloader)):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=data[\"input_ids\"].to(device),\n",
    "                attention_mask=data[\"attention_mask\"].to(device),\n",
    "                token_type_ids=data[\"token_type_ids\"].to(device),\n",
    "            )\n",
    "        logits = outputs[0]\n",
    "        prob = F.softmax(logits, dim=-1).detach().cpu().numpy()\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        result = np.argmax(logits, axis=-1)\n",
    "\n",
    "        output_pred.append(result)\n",
    "        output_prob.append(prob)\n",
    "\n",
    "    return (\n",
    "        np.concatenate(output_pred).tolist(),\n",
    "        np.concatenate(output_prob, axis=0).tolist(),\n",
    "    )\n",
    "\n",
    "def load_dataloader(\n",
    "    dataloder_type: str, data_path: pathlib.Path, tokenizer: AutoTokenizer\n",
    "):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        dataloder_type (str) : 가져올 dataloder 클래스 입니다 config 확인\n",
    "        load_data (pathlib.Path): 가져올 데이터의 주소입니다.\n",
    "        tokenizer (AutoTokenizer): 데이터를 토큰화할 토크나이저입니다.\n",
    "\n",
    "    Returns:\n",
    "        dataloader class : dataloader_type에 맞는 class 반환 합니다\n",
    "    \"\"\"\n",
    "    dataloader_config = {\n",
    "        \"BaseDataLoader\": BaseDataLoader(data_path, tokenizer),\n",
    "    }\n",
    "    return dataloader_config[dataloder_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--load model --- \n"
     ]
    }
   ],
   "source": [
    "#모델 로드\n",
    "import torch\n",
    "\n",
    "model_name = 'klue/roberta-large'\n",
    "load_model = '../dataset/best_model/klue_roberta-large/3'\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(load_model)\n",
    "model.to(device)\n",
    "print('--load model --- ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PERSON', 'LOCATION', 'CIVILIZATION', 'ARTIFACT', 'DATE', 'ORGANIZATION']\n"
     ]
    }
   ],
   "source": [
    "test_id, test_dataset, test_label , pre_sentence, pre_sub_word, pre_obj_word = load_dataloader(\n",
    "    'BaseDataLoader', '../dataset/train/train.csv', tokenizer\n",
    ").get_test_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 354/354 [02:08<00:00,  2.75it/s]\n"
     ]
    }
   ],
   "source": [
    "pred_answer, output_prob = inference(\n",
    "        model, test_dataset, 32, device\n",
    "    )  # model에서 class 추론\n",
    "\n",
    "pred_answer = num_to_label(pred_answer)  # 숫자로 된 class를 원래 문자열 라벨로 변환.\n",
    "\n",
    "## make csv file with predicted answer\n",
    "#########################################################\n",
    "\n",
    "# 아래 directory와 columns의 형태는 지켜주시기 바랍니다.\n",
    "output = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": test_id,\n",
    "        \"sentence\" : pre_sentence,\n",
    "        \"sub_word\" : pre_sub_word,\n",
    "        \"obj_word\" : pre_obj_word,\n",
    "        \"pred_label\": pred_answer,\n",
    "        \"probs\": output_prob,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11323\n",
      "1179 1900년, 의화단의 난이 일어나 청나라 조정이 열강에 선전 포고를 했을 때에, 이홍장, 장지동, 유곤일 등 지방 총독은 열강과 “동남상호협정”을 맺고 중앙의 명령을 무시했지만, 성선회가 이것을 정리하였다. | 유곤일 | 청나라\n",
      "1183 1900년, 의화단의 난이 일어나 청나라 조정이 열강에 선전 포고를 했을 때에, 이홍장, 장지동, 유곤일 등 지방 총독은 열강과 “동남상호협정”을 맺고 중앙의 명령을 무시했지만, 성선회가 이것을 정리하였다. | 이홍장 | 청나라\n",
      "1782 리사 디 배나는 중국에서 개최된 2007년 FIFA 여자 월드컵에서 노르웨이와의 조별 예선 경기(오스트레일리아는 노르웨이와 1-1 무승부를 기록함)에서 1골, 가나와의 조별 예선 경기(오스트레일리아는 가나에 4-1 승리를 기록함)에서 2골, 브라질과의 8강전 경기(오스트레일리아는 브라질에 2-3으로 패배함)에서 1골을 기록하여 오스트레일리아의 8강 진출에 기여했다. | 오스트레일리아 | 가나\n",
      "1783 리사 디 배나는 중국에서 개최된 2007년 FIFA 여자 월드컵에서 노르웨이와의 조별 예선 경기(오스트레일리아는 노르웨이와 1-1 무승부를 기록함)에서 1골, 가나와의 조별 예선 경기(오스트레일리아는 가나에 4-1 승리를 기록함)에서 2골, 브라질과의 8강전 경기(오스트레일리아는 브라질에 2-3으로 패배함)에서 1골을 기록하여 오스트레일리아의 8강 진출에 기여했다. | 오스트레일리아 | 리사 디 배나\n",
      "2076 헌강왕(憲康王, ~ 886년, 재위: 875년 ~ 886년)은 신라의 제49대 왕이다. | 헌강왕 | 신라\n",
      "2835 바투의 형제 베르케하에서, 킵차크 칸국은 베르크 칸이 바그다드 전투와 칼리프 알 무스타으심을 살해한 것으로 경멸했던 훌라구 칸이 다스린 일 칸국의 친척들과 분쟁에 휩싸인다. | 바투 | 킵차크 칸국\n",
      "2839 바투의 형제 베르케하에서, 킵차크 칸국은 베르크 칸이 바그다드 전투와 칼리프 알 무스타으심을 살해한 것으로 경멸했던 훌라구 칸이 다스린 일 칸국의 친척들과 분쟁에 휩싸인다. | 킵차크 칸국 | 베르크 칸\n",
      "3002 유한굉(劉漢宏, Liu Hanhong, ~ 887년)은 중국 당나라 말기에 활약했던 군벌로, 당초에는 당나라에 반기를 들었으나, 후에 당나라의 관직을 받고 의승군 절도사(義勝軍節度使, 본거지는 지금의 저장 성 사오싱 시)로서 절강 동부 일대를 지배하였다. | Hanhon | 당나라\n",
      "3244 신의손(申宜孫, 1960년 1월 12일 ~)은 러시아계 한국인 축구 선수로, 원래 이름은 발레리 콘스탄치노비치 사리체프(Valeriy Konstatinovich Sarychev)이며 소비에트 연방 타지크 소비에트 사회주의 공화국(현재 타지키스탄) 두샨베 출신이다. | 발레리 콘스탄치노비치 사리체프 | 타지크 소비에트 사회주의 공화국\n",
      "3245 신의손(申宜孫, 1960년 1월 12일 ~)은 러시아계 한국인 축구 선수로, 원래 이름은 발레리 콘스탄치노비치 사리체프(Valeriy Konstatinovich Sarychev)이며 소비에트 연방 타지크 소비에트 사회주의 공화국(현재 타지키스탄) 두샨베 출신이다. | 신의손 | 타지크 소비에트 사회주의 공화국\n",
      "4026 미하일 8세 팔레올로고스(1223년 – 1282년 12월 11일)는 비잔티움 제국의 마지막 왕조인 팔레올로고스 왕조의 황제(재위기간:1261년~1282년)이다. | 팔레올로고스 왕조 | 1261년~1282년\n",
      "4029 미하일 8세 팔레올로고스(1223년 – 1282년 12월 11일)는 비잔티움 제국의 마지막 왕조인 팔레올로고스 왕조의 황제(재위기간:1261년~1282년)이다. | 팔레올로고스 | 미하일\n",
      "4153 전쟁은 일본이 개항하면서 에도 막부(도쿠가와 막부)의 이이 나오스케(히코네 번)가 고메이 천황의 칙허도 없이 5개의 국가(미국, 네덜란드, 러시아, 영국, 프랑스)과의 안세이 5개국 조약을 체결하였다는 불평등 조약으로 인한 반막부 세력의 불만에서 기원되었다. | 히코네 번 | 이이 나오스케\n",
      "4834 당시 충숙왕과 가장 먼저 결혼한 부인은 공원왕후였음에도, 훗날 충숙왕과 결혼한 복국장공주나 조국장공주등이 고려 왕실에 들어오면서 홍씨는 종실 정안공의 집으로 나가서 지내야 했다. | 조국장 | 고려\n",
      "5822 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 구시 칸 | 오이라트부\n",
      "5826 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 구시 칸 | 코슈트 칸국\n",
      "5827 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 구시 칸 | 코슈트\n",
      "5833 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 토르바이크 | 코슈트 칸국\n",
      "5834 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 토르바이크 | 코슈트\n",
      "5838 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 그시리 칸 | 코슈트 칸국\n",
      "5839 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 그시리 칸 | 코슈트\n",
      "5843 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 코슈트 칸국 | 코슈트\n",
      "5844 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 코슈트 칸국 | 창업군주\n",
      "5845 구시 칸(1582년 ~ 1655년 1월 14일)은 별명 그시리 칸, 본명 토르바이크(Трбайх, Torbaikh Khan)는 17세기 중반 몽골 오이라트부의 네 씨족 중 하나인 코슈트의 귀족으로, 코슈트 칸국의 창업군주다. | 코슈트 칸국 | 1582년 ~ 1655년 1월 14일\n",
      "5944 고구려 장수왕(재위: 412년 음력 10월~491년 음력 12월)의 군사들이 한성으로 남침해 개로왕이 살해(475년) 되고, 왕자인 문주왕(재위 475년~477년)은 위기에 빠진 백제를 구하기 위한 숱한 고민 끝에, 목만치와 함께 남쪽으로 갔다. | 장수왕 | 고구려\n",
      "5962 고구려 장수왕(재위: 412년 음력 10월~491년 음력 12월)의 군사들이 한성으로 남침해 개로왕이 살해(475년) 되고, 왕자인 문주왕(재위 475년~477년)은 위기에 빠진 백제를 구하기 위한 숱한 고민 끝에, 목만치와 함께 남쪽으로 갔다. | 개로왕 | 백제\n",
      "5966 고구려 장수왕(재위: 412년 음력 10월~491년 음력 12월)의 군사들이 한성으로 남침해 개로왕이 살해(475년) 되고, 왕자인 문주왕(재위 475년~477년)은 위기에 빠진 백제를 구하기 위한 숱한 고민 끝에, 목만치와 함께 남쪽으로 갔다. | 문주왕 | 백제\n",
      "6279 푸블리우스 아일리우스 트라야누스 하드리아누스(76년 1월 24일~138년 7월 10일)는 로마 제국의 제14대 황제(재위: 117년~138년)이다. | 푸블리우스 아일리우스 트라야누스 하드리아누스 | 로마 제국\n",
      "6283 이후 신바빌로니아는 기원전 604년 경에 네부카드네자르 2세에 의해 이스라엘 왕국을 정복하고 바빌론의 문과 바빌론의 공중정원 등의 건축물들을 건설하는 등 최전성기를 맞이하였지만, 네부카드네자르 2세 사후 쇠퇴하기 시작한 신바빌로니아는 결국 기원전 539년에 동쪽에서 힘을 키워온 아케메네스 제국의 키루스 2세에게 멸망했다. | 네부카드네자르 | 신바빌로니아\n",
      "6286 이후 신바빌로니아는 기원전 604년 경에 네부카드네자르 2세에 의해 이스라엘 왕국을 정복하고 바빌론의 문과 바빌론의 공중정원 등의 건축물들을 건설하는 등 최전성기를 맞이하였지만, 네부카드네자르 2세 사후 쇠퇴하기 시작한 신바빌로니아는 결국 기원전 539년에 동쪽에서 힘을 키워온 아케메네스 제국의 키루스 2세에게 멸망했다. | 키루스 | 아케메네스 제국\n",
      "6437 나중에는 도조 히데키의 일본 제국 정부와도 손을 잡아 추축국을 만들었다. | 도조 히데키 | 일본 제국\n",
      "6954 산탄젤로 성 또는 하드리아누스의 영묘는 로마에 있는 원통 모양의 건축물로 원래는 로마 제국의 황제 하드리아누스가 자신과 가족을 위해 세운 무덤이었다. | 하드리아누스 | 로마 제국\n",
      "7304 연남생은 또다시 대형 염유(冉有)를 다시 당나라에 보내 구원을 청하였으나 회답이 없자, 이번에는 아들 연헌성(獻誠)을 당나라에 보내어 거듭 구원을 청하였다. | 당나라 | 연헌성\n",
      "7549 바오다이(保大, 1913년 10월 22일 ~ 1997년 7월 31일)는 베트남 응우옌 왕조의 마지막 제13대 황제(재위: 1925년 11월 13일(정식 즉위는 1926년 1월 8일)(정식 즉위는 1926년 1월 8일) ~ 1945년 3월 11일)이자 베트남 제국의 황제(재위: 1945년 3월 11일 ~ 1945년 8월 23일)이고 베트남국(베트남 공화국)의 국가원수(재임: 1949년 6월 13일 ~ 1955년 4월 30일)이다. | 응우옌 왕조 | 바오다이\n",
      "7552 바오다이(保大, 1913년 10월 22일 ~ 1997년 7월 31일)는 베트남 응우옌 왕조의 마지막 제13대 황제(재위: 1925년 11월 13일(정식 즉위는 1926년 1월 8일)(정식 즉위는 1926년 1월 8일) ~ 1945년 3월 11일)이자 베트남 제국의 황제(재위: 1945년 3월 11일 ~ 1945년 8월 23일)이고 베트남국(베트남 공화국)의 국가원수(재임: 1949년 6월 13일 ~ 1955년 4월 30일)이다. | 응우옌 왕조 | 베트남 제국\n",
      "7558 바오다이(保大, 1913년 10월 22일 ~ 1997년 7월 31일)는 베트남 응우옌 왕조의 마지막 제13대 황제(재위: 1925년 11월 13일(정식 즉위는 1926년 1월 8일)(정식 즉위는 1926년 1월 8일) ~ 1945년 3월 11일)이자 베트남 제국의 황제(재위: 1945년 3월 11일 ~ 1945년 8월 23일)이고 베트남국(베트남 공화국)의 국가원수(재임: 1949년 6월 13일 ~ 1955년 4월 30일)이다. | 바오다이 | 베트남 제국\n",
      "7989 비록 수도 평양이 함락되며 보장왕이 항복하고 고구려는 멸망했지만 아직 안시성을 비롯한 요동지역의 많은 성을 비롯하여 여러 곳에서 당에 항복하지 않고 아직 항전하고 있었고, 특히 고연무,검모잠,고정문 등의 각지의 고구려 지도자들은 구국 운동을 벌이며 치열한 대당 항쟁을 하게 된다. | 검모잠 | 고구려\n",
      "8369 람베르토 2세(Lambert II of Spoleto, 870년/875년경 - 898년 10월 15일)는 카롤링거 왕조 출신의 스폴레토 공작, 이탈리아의 군주이다. | 스폴레토 | 카롤링거 왕조\n",
      "8372 람베르토 2세(Lambert II of Spoleto, 870년/875년경 - 898년 10월 15일)는 카롤링거 왕조 출신의 스폴레토 공작, 이탈리아의 군주이다. | 람베르토 | 카롤링거 왕조\n",
      "8571 가타오카 시치로(1854년 1월 12일-1920년 1월 11일)는 일본 제국 해군의 군인으로 최종 계급은 제독이다. | 일본 제국 | 가타오카 시치로\n",
      "9138 역사가들은 율리우스 카이사르가 종신 독재관에 오른 기원전 44년, 마르쿠스 안토니우스가 악티움 해전에서 패배한 기원전 31년, 로마 원로원이 아우구스투스에게 특별한 권력을 부여한 기원전 27년 등을 로마 공화국이 종식된 시점으로 제시한다. | 아우구스투스 | 로마 공화국\n",
      "10239 양만춘(梁萬春/楊萬春, ~)은 고구려 말의 군인이다. | 양만춘 | 고구려 말\n",
      "10880 서울대학교 국사학과 교수인 송기호는 발해에게서 이러한 고구려 계승 의식이 표출된 이유는 여러 정황상 대조영은 말갈족이지만, 고구려에 귀속되어, 일정부분 고구려화 되었고, 걸걸중상을 거치면서 더욱 가속화되어 말갈계 고구려인으로서, 고구려 귀속의식이 나타나게 되어 훗날 발해국을 운영하는 기지가 되었다고 주장했다. | 발해국 | 고구려인\n",
      "10881 서울대학교 국사학과 교수인 송기호는 발해에게서 이러한 고구려 계승 의식이 표출된 이유는 여러 정황상 대조영은 말갈족이지만, 고구려에 귀속되어, 일정부분 고구려화 되었고, 걸걸중상을 거치면서 더욱 가속화되어 말갈계 고구려인으로서, 고구려 귀속의식이 나타나게 되어 훗날 발해국을 운영하는 기지가 되었다고 주장했다. | 발해국 | 말갈족\n",
      "10910 푸블리우스 아일리우스 트라야누스 하드리아누스(76년 1월 24일~138년 7월 10일)는 로마 제국의 제14대 황제(재위: 117년~138년)이다. | 푸블리우스 아일리우스 트라야누스 하드리아누스 | 로마 제국\n",
      "11200 3월, 사찬 설오유(薛烏儒)가 고구려 태대형 고연무와 함께 각각 병사 1만 씩을 거느리고 압록강을 건너서 옥골(屋骨)에 이르렀다. | 사찬 설오유 | 고구려\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "print(len(output))\n",
    "mask1 = (output.pred_label == 'per:origin')\n",
    "\n",
    "output_mask1 = output.loc[mask1,:]\n",
    "for idx, item  in output_mask1[:].iterrows() :\n",
    "    print(idx, item['sentence'] ,'|' ,item['sub_word'] , '|', item['obj_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_mask1.to_csv('./mask2.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
